{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 13/Set até às 23:59.<br />\n",
    "Grupo: 1 ou 2 pessoas.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO disponibilizar o arquivo com os *access keys/tokens* do Twitter.**\n",
    "\n",
    "\n",
    "### Check 3: \n",
    "\n",
    "Até o dia 06 de Setembro às 23:59, o notebook e o xlsx devem estar no Github com as seguintes evidências: \n",
    "    * Conta no twitter criada.\n",
    "    * Produto escolhido.\n",
    "    * Arquivo Excel contendo a base de treinamento e teste já classificado.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. Esteja livre para adicionar outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "* Conta: *** @Rebecamoreno_***\n",
    "\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n",
    "\n",
    "**ATENÇÃO**: Nunca divulgue os dados desse arquivo online (GitHub, etc). Ele contém as chaves necessárias para realizar as operações no twitter de forma automática e portanto é equivalente a ser \"hackeado\". De posse desses dados, pessoas mal intencionadas podem fazer todas as operações manuais (tweetar, seguir, bloquear/desbloquear, listar os seguidores, etc). Para efeito do projeto, esse arquivo não precisa ser entregue!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @Rebecamoreno_\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Nutella'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "Não se esqueça de colocar um nome para a coluna na célula **B1**.<br /><br />\n",
    "Fazer o mesmo na planilha de Controle.\n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Escreva o seu código abaixo:\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames para relevante e não é relevante separados (na planilha treinamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_excel('./{0}.xlsx'.format(produto),sheet='Treinamento')\n",
    "\n",
    "dftsim = dft[(dft.Relevante==\"Sim\")]\n",
    "dftnao = dft[(dft.Relevante==\"Não\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame para a planilha de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lendo = pd.ExcelFile('./{0}.xlsx'.format(produto))\n",
    "dfteste = lendo.parse(\"Teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função que limpa os tweets (remove vírgulas, aspas, nomes dos usuários,...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limpadorcaracter(frase):\n",
    "    #rodando para cada palavra da frase recebida \n",
    "    for palavra in frase.split():\n",
    "        #rodando para cada letra da palavra\n",
    "        for letras in palavra:\n",
    "            if letras == '@' or 'https' in palavra or 'kk' in palavra or 'kkk' in palavra:\n",
    "                frase = frase.replace(palavra,\"\").replace('  ',' ')\n",
    "            if not '@' in palavra and (letras == (\",\") or letras == \"'\" or letras == '\"' or letras == '[' or letras == '‹' or letras == ']' or letras == '#' or letras == ':' or letras == ';' or letras == '›' or letras == '!' or letras == '(' or letras == ')' or letras == '/' or letras == '\\n' or letras == '.' or letras == '\\\\' or letras == '-' or letras == '$' or letras == '%' or letras == '|' or letras == '=' or letras == '*' or letras == 'ˆ' or letras == '&' or letras == '+' or letras == ('?')):\n",
    "                palavra = palavra.replace(letras,\" \")\n",
    "                frase = frase.replace(letras,\" \")\n",
    "    return frase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contagem do total de palavras nos tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como curiosidade, a palavra que aparece com mais frequência em toda essa análise é a palavra \"nutella\", com uma frenquência de 228 vezes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = {}     #Dicionario com as palavras dos tweets\n",
    "todas = 0  #Contador das palavras no total \n",
    "c=0\n",
    "\n",
    "for msg in dft[\"Treinamento\"]:\n",
    "    frase = limpadorcaracter(msg)\n",
    "    for i in frase.split():\n",
    "        #analisando cada palavra ja separada\n",
    "        #contando sua frenquencia e adicionando em A\n",
    "        if not i in A:\n",
    "            A[i]=0\n",
    "        A[i]+=1\n",
    "        todas+=1\n",
    "        if A[i]>=c:\n",
    "            c=A[i]\n",
    "            string = i\n",
    "\n",
    "print('Como curiosidade, a palavra que aparece com mais frequência em toda essa análise é a palavra \"{0}\", com uma frenquência de {1} vezes.\\n'.format(string,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contagem do total de palavras relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de RELEVANTES é a palavra \"nutella\", com uma frenquência de 140 vezes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "S = {}            #Dicionario com as palavras em relevantes dos tweets\n",
    "todas_em_sim = 0  #Contador das palavras no sim\n",
    "p = 0\n",
    "\n",
    "for msg in dftsim[\"Treinamento\"]:\n",
    "    frase = limpadorcaracter(msg)\n",
    "    for i in frase.split():\n",
    "        #analisando cada palavra ja limpa\n",
    "        #contando sua frenquencia e adicionando em S\n",
    "        if not i in S:\n",
    "            S[i]=0\n",
    "        S[i]+=1\n",
    "        todas_em_sim+=1\n",
    "        if S[i]>=p:\n",
    "            p=S[i]\n",
    "            strings = i\n",
    "            \n",
    "print('Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de RELEVANTES é a palavra \"{0}\", com uma frenquência de {1} vezes.\\n'.format(strings,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contagem do total de palavras não relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de NÃO RELEVANTES é a palavra \"nutella\", com uma frenquência de 88 vezes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = {}            #Dicionario com as palavras em não relevantes dos tweets\n",
    "todas_em_nao = 0  #Contador das palavras no não\n",
    "f = 0\n",
    "\n",
    "for msg in dftnao[\"Treinamento\"]:\n",
    "    frase = limpadorcaracter(msg)\n",
    "    for i in frase.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        #contando sua frenquencia e adicionando em N\n",
    "        if not i in N:\n",
    "            N[i]=0\n",
    "        N[i]+=1\n",
    "        todas_em_nao+=1\n",
    "        if N[i]>=f:\n",
    "            f=N[i]\n",
    "            stringn = i\n",
    "                        \n",
    "print('Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de NÃO RELEVANTES é a palavra \"{0}\", com uma frenquência de {1} vezes.\\n'.format(stringn,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quantidade TOTAL de palavras no UNIVERSO é 3601.\n",
      "A quantidade TOTAL de palavras dentro de RELEVANTES é 1691.\n",
      "A quantidade TOTAL de palavras dentro de NÃO RELEVANTES é 1910.\n",
      "\n",
      "A quantidade de palavras DIFERENTES no UNIVERSO é 1176.\n",
      "A quantidade de palavras DIFERENTES dentro de RELEVANTES é 578.\n",
      "A quantidade de palavras DIFERENTES dentro de NÃO RELEVANTES é 783.\n"
     ]
    }
   ],
   "source": [
    "print('A quantidade TOTAL de palavras no UNIVERSO é {0}.'.format(todas))\n",
    "print('A quantidade TOTAL de palavras dentro de RELEVANTES é {0}.'.format(todas_em_sim))\n",
    "print('A quantidade TOTAL de palavras dentro de NÃO RELEVANTES é {0}.\\n'.format(todas_em_nao))\n",
    "print('A quantidade de palavras DIFERENTES no UNIVERSO é {0}.'.format(len(A)))\n",
    "print('A quantidade de palavras DIFERENTES dentro de RELEVANTES é {0}.'.format(len(S)))\n",
    "print('A quantidade de palavras DIFERENTES dentro de NÃO RELEVANTES é {0}.'.format(len(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo da probabilidade de cada palavra em relevante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conta: P(sim|palavra) = (P(palavra|sim)+1) / P(palavras totais|sim)+P(palavras totais sem repetição) = (S[i]+1)/bs+c\n",
    "bs = todas_em_sim\n",
    "bn = todas_em_nao\n",
    "c = len(A)\n",
    "PS = {}\n",
    "PN = {}\n",
    "TSim = dftsim[\"Treinamento\"].count()\n",
    "TNao = dftnao[\"Treinamento\"].count()\n",
    "TTotal = dft[\"Treinamento\"].count()\n",
    "ListaPtweetS=[]\n",
    "ListaPtweetN=[]\n",
    "Resultado=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-89f918d84bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdfteste\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Resultado\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResultado\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\gb_pa\\Documents\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2417\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\gb_pa\\Documents\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\gb_pa\\Documents\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2657\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\gb_pa\\Documents\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Length of values does not match length of '\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPeriodIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "for msg2 in dfteste['Teste']:\n",
    "    #Para sim:\n",
    "    PtweetS = (TSim/TTotal)\n",
    "    for i in msg2.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        for caracter in range(0,len(i)):\n",
    "            i = i.strip(\",\").strip(\"'\").strip('\"').strip('#').strip(':').strip(';').strip('!').strip('(').strip(')').strip('/').strip('\\n').strip('.').strip('\\\\').strip('-').strip('$').strip('%').strip('|').strip('=').strip('*').strip('ˆ').strip('&').strip('+').strip('?')\n",
    "        #contando sua frenquencia e adicionando em S\n",
    "        \n",
    "        if i in S:\n",
    "            PS[i] = (S[i]+1)/(bs+c)\n",
    "            \n",
    "        else:\n",
    "            PS[i] = 1/(bs+c)\n",
    "            \n",
    "        PtweetS = PtweetS*PS[i]\n",
    "    ListaPtweetS.append(PtweetS)\n",
    "        \n",
    "    #Para não:\n",
    "    PtweetN = (TNao/TTotal)\n",
    "    for i in msg2.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        for caracter in range(0,len(i)):\n",
    "            i = i.strip(\",\").strip(\"'\").strip('\"').strip('#').strip(':').strip(';').strip('!').strip('(').strip(')').strip('/').strip('\\n').strip('.').strip('\\\\').strip('-').strip('$').strip('%').strip('|').strip('=').strip('*').strip('ˆ').strip('&').strip('+').strip('?')\n",
    "        #contando sua frenquencia e adicionando em N\n",
    "        \n",
    "        if i in N:\n",
    "            PN[i] = (N[i]+1)/(bn+c)\n",
    "            \n",
    "        else:\n",
    "            PN[i] = 1/(bn+c)\n",
    "        \n",
    "        PtweetN = PtweetN*PN[i]\n",
    "    ListaPtweetN.append(PtweetN)\n",
    "        \n",
    "for i in range(len(ListaPtweetS)-1):\n",
    "    #print(ListaPtweetS[i])\n",
    "    #print(ListaPtweetN[i])\n",
    "    if ListaPtweetS[i]>ListaPtweetN[i]:\n",
    "        Resultado.append(\"Sim\")\n",
    "    if ListaPtweetN[i]>ListaPtweetS[i]:\n",
    "        Resultado.append(\"Não\")\n",
    "\n",
    "\n",
    "dfteste[\"Resultado\"] = Resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Opcionalmente:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Igual = 0\n",
    "Diferente = 0\n",
    "for i in range(dfteste[\"Teste\"].count()):\n",
    "    if dfteste[\"Relevante\"][i] == dfteste[\"Resultado\"][i]:\n",
    "        Igual+=1\n",
    "    else:\n",
    "        Diferente+=1\n",
    "        \n",
    "print(Igual)\n",
    "print(Diferente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo as variaveis para confirmar os positivos e negativos\n",
    "PosFalso = 0\n",
    "PosVer = 0\n",
    "NegVer = 0\n",
    "NegFalso = 0\n",
    "\n",
    "for i in range(dfteste[\"Teste\"].count()):\n",
    "    #confirmando\n",
    "    if dfteste[\"Relevante\"][i]=='Não' and dfteste[\"Resultado\"][i]=='Sim':\n",
    "        PosFalso+=1\n",
    "    if dfteste[\"Relevante\"][i]=='Sim' and dfteste[\"Resultado\"][i]=='Sim':\n",
    "        PosVer+=1\n",
    "    if dfteste[\"Relevante\"][i]=='Não' and dfteste[\"Resultado\"][i]=='Não':\n",
    "        NegVer+=1\n",
    "    if dfteste[\"Relevante\"][i]=='Sim' and dfteste[\"Resultado\"][i]=='Não':\n",
    "        NegFalso+=1\n",
    "        \n",
    "print(PosFalso)\n",
    "print(PosVer)\n",
    "print(NegVer)\n",
    "print(NegFalso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PeN = []\n",
    "\n",
    "for i in range(dfteste[\"Teste\"].count()):\n",
    "    #confirmando\n",
    "    if dfteste[\"Relevante\"][i]=='Não' and dfteste[\"Resultado\"][i]=='Sim':\n",
    "        PosFalso+=1\n",
    "        PeN.append('PositivoFalso')\n",
    "\n",
    "    if dfteste[\"Relevante\"][i]=='Sim' and dfteste[\"Resultado\"][i]=='Sim':\n",
    "        PosVer+=1\n",
    "        PeN.append('PositivoVerdadeiro')\n",
    "        \n",
    "    if dfteste[\"Relevante\"][i]=='Não' and dfteste[\"Resultado\"][i]=='Não':\n",
    "        NegVer+=1\n",
    "        PeN.append('NegativoVerdadeiro')\n",
    "        \n",
    "    if dfteste[\"Relevante\"][i]=='Sim' and dfteste[\"Resultado\"][i]=='Não':\n",
    "        NegFalso+=1\n",
    "        PeN.append('NegativoFalso')\n",
    "        #relevante\n",
    "        \n",
    "dfteste[\"pn\"] = PeN\n",
    "dfteste\n",
    "\n",
    "\n",
    "k = dfteste.pn.value_counts()/len(PeN)*100\n",
    "s = k.reindex(['PositivoFalso','PositivoVerdadeiro','NegativoVerdadeiro','NegativoFalso'])\n",
    "plot = s.plot(kind='pie',title='Positivos e Negativos',autopct='%.1f',figsize=(6, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
