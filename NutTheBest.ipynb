{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Automático de Sentimento\n",
    "\n",
    "Você foi contratado por uma empresa parar analisar como os clientes estão reagindo a um determinado produto no Twitter. A empresa deseja que você crie um programa que irá analisar as mensagens disponíveis e classificará como \"relevante\" ou \"irrelevante\". Com isso ela deseja que mensagens negativas, que denigrem o nome do produto, ou que mereçam destaque, disparem um foco de atenção da área de marketing.<br /><br />\n",
    "Como aluno de Ciência dos Dados, você lembrou do Teorema de Bayes, mais especificamente do Classificador Naive-Bayes, que é largamente utilizado em filtros anti-spam de e-mails. O classificador permite calcular qual a probabilidade de uma mensagem ser relevante dadas as palavras em seu conteúdo.<br /><br />\n",
    "Para realizar o MVP (*minimum viable product*) do projeto, você precisa implementar uma versão do classificador que \"aprende\" o que é relevante com uma base de treinamento e compara a performance dos resultados com uma base de testes.<br /><br />\n",
    "Após validado, o seu protótipo poderá também capturar e classificar automaticamente as mensagens da plataforma.\n",
    "\n",
    "## Informações do Projeto\n",
    "\n",
    "Prazo: 13/Set até às 23:59.<br />\n",
    "Grupo: 1 ou 2 pessoas.<br /><br />\n",
    "Entregáveis via GitHub: \n",
    "* Arquivo notebook com o código do classificador, seguindo as orientações abaixo.\n",
    "* Arquivo Excel com as bases de treinamento e teste totalmente classificado.\n",
    "\n",
    "**NÃO disponibilizar o arquivo com os *access keys/tokens* do Twitter.**\n",
    "\n",
    "\n",
    "### Check 3: \n",
    "\n",
    "Até o dia 06 de Setembro às 23:59, o notebook e o xlsx devem estar no Github com as seguintes evidências: \n",
    "    * Conta no twitter criada.\n",
    "    * Produto escolhido.\n",
    "    * Arquivo Excel contendo a base de treinamento e teste já classificado.\n",
    "\n",
    "Sugestão de leitura:<br />\n",
    "http://docs.tweepy.org/en/v3.5.0/index.html<br />\n",
    "https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Preparando o ambiente\n",
    "\n",
    "Instalando a biblioteca *tweepy* para realizar a conexão com o Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as Bibliotecas que serão utilizadas. Esteja livre para adicionar outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "Para realizar a captura dos dados é necessário ter uma conta cadastrada no twitter:\n",
    "\n",
    "* Conta: *** @Rebecamoreno_***\n",
    "\n",
    "\n",
    "1. Caso ainda não tenha uma: https://twitter.com/signup\n",
    "1. Depois é necessário registrar um app para usar a biblioteca: https://apps.twitter.com/\n",
    "1. Dentro do registro do App, na aba Keys and Access Tokens, anotar os seguintes campos:\n",
    "    1. Consumer Key (API Key)\n",
    "    1. Consumer Secret (API Secret)\n",
    "1. Mais abaixo, gere um Token e anote também:\n",
    "    1. Access Token\n",
    "    1. Access Token Secret\n",
    "    \n",
    "1. Preencha os valores no arquivo \"auth.pass\"\n",
    "\n",
    "**ATENÇÃO**: Nunca divulgue os dados desse arquivo online (GitHub, etc). Ele contém as chaves necessárias para realizar as operações no twitter de forma automática e portanto é equivalente a ser \"hackeado\". De posse desses dados, pessoas mal intencionadas podem fazer todas as operações manuais (tweetar, seguir, bloquear/desbloquear, listar os seguidores, etc). Para efeito do projeto, esse arquivo não precisa ser entregue!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @Rebecamoreno_\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Coletando Dados\n",
    "\n",
    "Agora vamos coletar os dados. Tenha em mente que dependendo do produto escolhido, não haverá uma quantidade significativa de mensagens, ou ainda poder haver muitos retweets.<br /><br /> \n",
    "Configurando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Nutella'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "    msgs.append(msg.text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificando as Mensagens\n",
    "\n",
    "Agora você deve abrir o arquivo Excel com as mensagens capturadas e classificar na Coluna B se a mensagem é relevante ou não.<br /> \n",
    "Não se esqueça de colocar um nome para a coluna na célula **B1**.<br /><br />\n",
    "Fazer o mesmo na planilha de Controle.\n",
    "\n",
    "___\n",
    "## Montando o Classificador Naive-Bayes\n",
    "\n",
    "Com a base de treinamento montada, comece a desenvolver o classificador. Escreva o seu código abaixo:\n",
    "\n",
    "Opcionalmente: \n",
    "* Limpar as mensagens removendo os caracteres: enter, :, \", ', (, ), etc. Não remover emojis.<br />\n",
    "* Corrigir separação de espaços entre palavras e/ou emojis.\n",
    "* Propor outras limpezas/transformações que não afetem a qualidade da informação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_excel('./{0}.xlsx'.format(produto),sheet='Treinamento')\n",
    "\n",
    "dftsim = dft[(dft.Relevante==\"Sim\")]\n",
    "dftnao = dft[(dft.Relevante==\"Não\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lendo = pd.ExcelFile('./{0}.xlsx'.format(produto))\n",
    "dfteste = lendo.parse(\"Teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como curiosidade, a palavra que aparece com mais frequência em toda essa análise é a palavra \"nutella\", com uma frenquência de 219 vezes.\n",
      "\n",
      "Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de RELEVANTES é a palavra \"nutella\", com uma frenquência de 135 vezes.\n",
      "\n",
      "Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de NÃO RELEVANTES é a palavra \"nutella\", com uma frenquência de 84 vezes.\n",
      "\n",
      "A quantidade TOTAL de palavras no UNIVERSO é 3774.\n",
      "A quantidade TOTAL de palavras dentro de RELEVANTES é 1750.\n",
      "A quantidade TOTAL de palavras dentro de NÃO RELEVANTES é 2024.\n",
      "\n",
      "A quantidade de palavras DIFERENTES no UNIVERSO é 1370.\n",
      "A quantidade de palavras DIFERENTES dentro de RELEVANTES é 636.\n",
      "A quantidade de palavras DIFERENTES dentro de NÃO RELEVANTES é 917.\n"
     ]
    }
   ],
   "source": [
    "# A, S e N são dicionários com as frequências de cada uma das palavras separadas (splitadas).\n",
    "A = {}\n",
    "S = {}\n",
    "N = {}\n",
    "todas = 0\n",
    "c=0\n",
    "#para cada post (msg)\n",
    "for msg in dft[\"Treinamento\"]:\n",
    "    #para cada palavra dentro do post (i)\n",
    "    #msg = msg.split()\n",
    "    for i in msg.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        for caracter in range(0,len(i)):\n",
    "            i = i.strip(\",\").strip(\"'\").strip('\"').strip('#').strip(':').strip(';').strip('!').strip('(').strip(')').strip('/').strip('\\n').strip('.').strip('\\\\').strip('-').strip('$').strip('%').strip('|').strip('=').strip('*').strip('ˆ').strip('&').strip('+').strip('?')\n",
    "        #contando sua frenquencia e adicionando em A\n",
    "        if not i in A:\n",
    "            A[i]=0\n",
    "        A[i]+=1\n",
    "        todas+=1\n",
    "        if A[i]>=c:\n",
    "            c=A[i]\n",
    "            string = i\n",
    "\n",
    "print('Como curiosidade, a palavra que aparece com mais frequência em toda essa análise é a palavra \"{0}\", com uma frenquência de {1} vezes.\\n'.format(string,c))\n",
    "todas_em_sim = 0\n",
    "p = 0\n",
    "#para cada post (msg)\n",
    "for msg in dftsim[\"Treinamento\"]:\n",
    "    #para cada palavra dentro do post (i)\n",
    "    #msg = msg.split()\n",
    "    for i in msg.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        for caracter in range(0,len(i)):\n",
    "            i = i.strip(\",\").strip(\"'\").strip('\"').strip('#').strip(':').strip(';').strip('!').strip('(').strip(')').strip('/').strip('\\n').strip('.').strip('\\\\').strip('-').strip('$').strip('%').strip('|').strip('=').strip('*').strip('ˆ').strip('&').strip('+').strip('?')\n",
    "        #contando sua frenquencia e adicionando em A\n",
    "        if not i in S:\n",
    "            S[i]=0\n",
    "        S[i]+=1\n",
    "        todas_em_sim+=1\n",
    "        if S[i]>=p:\n",
    "            p=S[i]\n",
    "            strings = i\n",
    "            \n",
    "print('Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de RELEVANTES é a palavra \"{0}\", com uma frenquência de {1} vezes.\\n'.format(strings,p))\n",
    "todas_em_nao = 0\n",
    "f = 0\n",
    "#para cada post (msg)\n",
    "for msg in dftnao[\"Treinamento\"]:\n",
    "    #para cada palavra dentro do post (i)\n",
    "    #msg = msg.split()\n",
    "    for i in msg.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        for caracter in range(0,len(i)):\n",
    "            i = i.strip(\",\").strip(\"'\").strip('\"').strip('#').strip(':').strip(';').strip('!').strip('(').strip(')').strip('/').strip('\\n').strip('.').strip('\\\\').strip('-').strip('$').strip('%').strip('|').strip('=').strip('*').strip('ˆ').strip('&').strip('+').strip('?')\n",
    "        #contando sua frenquencia e adicionando em A\n",
    "        if not i in N:\n",
    "            N[i]=0\n",
    "        N[i]+=1\n",
    "        todas_em_nao+=1\n",
    "        if N[i]>=f:\n",
    "            f=N[i]\n",
    "            stringn = i\n",
    "            \n",
    "print('Como curiosidade, a palavra que aparece com mais frequência entre todas as palavras de NÃO RELEVANTES é a palavra \"{0}\", com uma frenquência de {1} vezes.\\n'.format(stringn,f))\n",
    "print('A quantidade TOTAL de palavras no UNIVERSO é {0}.'.format(todas))\n",
    "print('A quantidade TOTAL de palavras dentro de RELEVANTES é {0}.'.format(todas_em_sim))\n",
    "print('A quantidade TOTAL de palavras dentro de NÃO RELEVANTES é {0}.\\n'.format(todas_em_nao))\n",
    "print('A quantidade de palavras DIFERENTES no UNIVERSO é {0}.'.format(len(A)))\n",
    "print('A quantidade de palavras DIFERENTES dentro de RELEVANTES é {0}.'.format(len(S)))\n",
    "print('A quantidade de palavras DIFERENTES dentro de NÃO RELEVANTES é {0}.'.format(len(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sim', 'sim', 'nao', 'sim', 'sim', 'sim', 'nao', 'sim', 'nao', 'sim', 'nao', 'sim', 'sim', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'sim', 'sim', 'sim', 'nao', 'sim', 'sim', 'sim', 'nao', 'nao', 'sim', 'sim', 'sim', 'sim', 'sim', 'sim', 'nao', 'nao', 'sim', 'nao', 'sim', 'sim', 'sim', 'sim', 'sim', 'nao', 'nao', 'sim', 'nao', 'nao', 'sim', 'nao', 'sim', 'nao', 'nao', 'sim', 'sim', 'sim', 'sim', 'sim', 'sim', 'nao', 'sim', 'sim', 'sim', 'nao', 'sim', 'nao', 'sim', 'sim', 'nao', 'sim', 'nao', 'nao', 'sim', 'sim', 'sim', 'sim', 'nao', 'nao', 'sim', 'sim', 'sim', 'sim', 'nao', 'nao', 'nao', 'sim', 'nao', 'sim', 'sim', 'nao', 'nao', 'sim', 'nao', 'sim', 'sim', 'nao', 'sim', 'sim', 'nao', 'nao', 'sim', 'nao', 'nao', 'sim', 'nao', 'nao', 'sim', 'nao', 'sim', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'sim', 'sim', 'nao', 'sim', 'nao', 'sim', 'nao', 'sim', 'sim', 'sim', 'sim', 'sim', 'sim', 'sim', 'sim', 'sim', 'nao', 'nao', 'sim', 'sim', 'nao', 'sim', 'sim', 'nao', 'nao', 'sim', 'sim', 'nao', 'nao', 'nao', 'sim', 'sim', 'nao', 'sim', 'nao', 'sim', 'sim', 'sim', 'sim', 'nao', 'sim', 'sim', 'sim', 'nao', 'nao', 'nao', 'nao', 'nao', 'nao', 'sim', 'nao', 'nao', 'nao', 'nao', 'sim', 'nao', 'sim', 'sim', 'sim', 'nao', 'nao', 'sim', 'nao', 'nao', 'sim', 'nao', 'nao', 'nao', 'sim', 'sim', 'nao', 'nao', 'nao', 'nao', 'nao', 'sim', 'nao', 'nao', 'sim']\n"
     ]
    }
   ],
   "source": [
    "#Calculando a probabilidade de cada palavra\n",
    "#Conta: P(sim|palavra)=(P(palavra|sim)+1) / P(palavras totais|sim)+P(palavras totais sem repetição)\n",
    "bs = todas_em_sim\n",
    "bn = todas_em_nao\n",
    "c = len(A)\n",
    "PS = {}\n",
    "PN = {}\n",
    "TSim = dftsim[\"Treinamento\"].count()\n",
    "TNao = dftnao[\"Treinamento\"].count()\n",
    "TTotal = dft[\"Treinamento\"].count()\n",
    "ListaPtweetS=[]\n",
    "ListaPtweetN=[]\n",
    "Resultado=[]\n",
    "addmsgs = []\n",
    "#PTeste = {}\n",
    "\n",
    "for msg2 in dfteste['Teste']:\n",
    "    #Para sim:\n",
    "    PtweetS = (TSim/TTotal)\n",
    "    for i in msg2.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        for caracter in range(0,len(i)):\n",
    "            i = i.strip(\",\").strip(\"'\").strip('\"').strip('#').strip(':').strip(';').strip('!').strip('(').strip(')').strip('/').strip('\\n').strip('.').strip('\\\\').strip('-').strip('$').strip('%').strip('|').strip('=').strip('*').strip('ˆ').strip('&').strip('+').strip('?')\n",
    "        #contando sua frenquencia e adicionando em A\n",
    "        \n",
    "        if i in S:\n",
    "            PS[i] = (S[i]+1)/(bs+c)\n",
    "            \n",
    "        else:\n",
    "            PS[i] = 1/(bs+c)\n",
    "            \n",
    "        PtweetS = PtweetS*PS[i]\n",
    "    ListaPtweetS.append(PtweetS)\n",
    "    addmsgs.append(msg2)\n",
    "        \n",
    "    #Para não:\n",
    "    PtweetN = (TNao/TTotal)\n",
    "    for i in msg2.split():\n",
    "        #analisando cada palavra ja separada (for e if)\n",
    "        for caracter in range(0,len(i)):\n",
    "            i = i.strip(\",\").strip(\"'\").strip('\"').strip('#').strip(':').strip(';').strip('!').strip('(').strip(')').strip('/').strip('\\n').strip('.').strip('\\\\').strip('-').strip('$').strip('%').strip('|').strip('=').strip('*').strip('ˆ').strip('&').strip('+').strip('?')\n",
    "        #contando sua frenquencia e adicionando em A\n",
    "        \n",
    "        if i in N:\n",
    "            PN[i] = (N[i]+1)/(bn+c)\n",
    "            \n",
    "        else:\n",
    "            PN[i] = 1/(bn+c)\n",
    "        \n",
    "        PtweetN = PtweetN*PN[i]\n",
    "    ListaPtweetN.append(PtweetN)\n",
    "    #addmsgs.append(msg2)\n",
    "        \n",
    "for i in range(len(ListaPtweetS)):\n",
    "    #print(ListaPtweetS[i])\n",
    "    #print(ListaPtweetN[i])\n",
    "    if ListaPtweetS[i]>ListaPtweetN[i]:\n",
    "        Resultado.append(\"sim\")\n",
    "    if ListaPtweetN[i]>ListaPtweetS[i]:\n",
    "        Resultado.append(\"nao\")\n",
    "\n",
    "\n",
    "#dfteste[\"Resultado\"] = Resultado\n",
    "print(Resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>relevante</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @br00talgirl: espero um dia ser feliz igual...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adicionei um vídeo a uma playlist @youtube htt...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pedronic_ @sleepool @gabriel0h @prdww lucas n...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>preciso ir na casa do gui pra comer nutella kkj</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comecei ser fitness hj, comendo um bolo no pot...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nutella logo de manhã ❤️</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>passando a banana na nutella sem duplo sentido</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanto doce pra harmonizar e vcs insistem nessa...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rt @ac_bulhoes: essa sua pele preta, nutella, ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>queria comer outra nutella</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rt @matheussvini: as vezes o ser humano so pre...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rt @_falathay: tô comendo igual bicho</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>queria alguém só pra já saber quando tô de tpm...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rt @marketingcrf: 2017: cê acredita, voz de mo...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rt @sigaarmandinho: aceito ir pra um lugar que...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@coemoren4 ali em botafogo menina, esse lugar ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ter alguém que te ame de vdd deve ser do caral...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017: cê acredita, voz de mongoloide do mc kev...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@nutella_a_mais meu afilhado mais querido ❤️❤️❤️</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rt @nxjvffc: se voce gosta nao custa nada demo...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rt @crvgluuhf: a pessoa só começa a dar valor ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rt @_brunaa_silvaa_: @nutella_a_mais @xxevinha...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@luaracristal14 tem nutella aqui em casa</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>poucas coisas nesse mundo combinam tão bem qua...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>eu queria uma torta de leite ninho com nutella</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@danifields @gabrielfuh_ é a propaganda gigant...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sonhei que mh tinha feito uma mesa cheia de cr...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>coisas muito superestimadas vol1:\\n1: sexo ana...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@breendinha_15 com nutella pelo amor 🙏🏼</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@sportv hj é a geração nutella !</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>a mana ganhou um pote de nutella imenso do avô...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>parabéns, nutella!\\ntmj irmão! https://t.co/he...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>foto de gatinho e de cachorrinho sempre faz su...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>é muito engraçado o áudio do matheus descreven...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>rt @brenolemes2016: que mina que não gosta de ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>rt @renatotaioba: assistindo vídeos de receita...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>rt @saarahsaad: odiar ex é mt geração nutella ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>arthur fez um bolo e levou hoje pro trabalho, ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>rt @isasantana2403: só queria um morango com n...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>https://t.co/56amzinurr\\nquero muito um pote d...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>rt @alexmapelli0: já fui 10/10 com quem não me...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>eu sou a ex mais nutella que existe kkkkkkkkk</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>eu sou uma pessoa tão indecisa que tô comendo ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>rt @umvicente: iago sousa não está mais entre ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>gostei de um vídeo @youtube https://t.co/mzaqi...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>eu ganhei 2 bombons de leite ninho com nutella...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>rt @tmtisaac: adolescência nutella// adolescên...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>sdds do meu namorado n deu sinal de vida desde...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>rt @iioveviajar: japão 🇯🇵 https://t.co/ywg73hpglr</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>vou fazer pro gabriel de brigadeiro de leite n...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>comi o resto da nutella que tinha aqui, amanhã...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>@kirchprincess vou hitar mais que o irmão do f...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>rt @09h08_: entao vamos viver, e um dia a gent...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>rt @viniceow: mais um dia sem achar o rumo da ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>rt @ac_bulhoes: essa sua pele preta, nutella, ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>@aguinaldinho pensamento retrógrado é você cri...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>rt @portalechociay: um pouco de pote de nutell...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>imagina se eu fizesse textão no facebook toda ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rt @califaphotos: beijar só fortalece a amizad...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>achei dez conto na rua hoje, amanhã dá pra eu ...</td>\n",
       "      <td>sim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste relevante\n",
       "0    rt @br00talgirl: espero um dia ser feliz igual...       sim\n",
       "1    adicionei um vídeo a uma playlist @youtube htt...       sim\n",
       "2    @pedronic_ @sleepool @gabriel0h @prdww lucas n...       sim\n",
       "3      preciso ir na casa do gui pra comer nutella kkj       sim\n",
       "4    comecei ser fitness hj, comendo um bolo no pot...       sim\n",
       "5                             nutella logo de manhã ❤️       sim\n",
       "6       passando a banana na nutella sem duplo sentido       sim\n",
       "7    tanto doce pra harmonizar e vcs insistem nessa...       sim\n",
       "8    rt @ac_bulhoes: essa sua pele preta, nutella, ...       sim\n",
       "9                           queria comer outra nutella       sim\n",
       "10   rt @matheussvini: as vezes o ser humano so pre...       sim\n",
       "11               rt @_falathay: tô comendo igual bicho       sim\n",
       "12   queria alguém só pra já saber quando tô de tpm...       sim\n",
       "13   rt @marketingcrf: 2017: cê acredita, voz de mo...       sim\n",
       "14   rt @sigaarmandinho: aceito ir pra um lugar que...       sim\n",
       "15   @coemoren4 ali em botafogo menina, esse lugar ...       sim\n",
       "16   ter alguém que te ame de vdd deve ser do caral...       sim\n",
       "17   2017: cê acredita, voz de mongoloide do mc kev...       sim\n",
       "18    @nutella_a_mais meu afilhado mais querido ❤️❤️❤️       sim\n",
       "19   rt @nxjvffc: se voce gosta nao custa nada demo...       sim\n",
       "20   rt @crvgluuhf: a pessoa só começa a dar valor ...       sim\n",
       "21   rt @_brunaa_silvaa_: @nutella_a_mais @xxevinha...       sim\n",
       "22            @luaracristal14 tem nutella aqui em casa       sim\n",
       "23   poucas coisas nesse mundo combinam tão bem qua...       sim\n",
       "24      eu queria uma torta de leite ninho com nutella       sim\n",
       "25   @danifields @gabrielfuh_ é a propaganda gigant...       sim\n",
       "26   sonhei que mh tinha feito uma mesa cheia de cr...       sim\n",
       "27   coisas muito superestimadas vol1:\\n1: sexo ana...       sim\n",
       "28             @breendinha_15 com nutella pelo amor 🙏🏼       sim\n",
       "29                    @sportv hj é a geração nutella !       sim\n",
       "..                                                 ...       ...\n",
       "170  a mana ganhou um pote de nutella imenso do avô...       sim\n",
       "171  parabéns, nutella!\\ntmj irmão! https://t.co/he...       sim\n",
       "172  foto de gatinho e de cachorrinho sempre faz su...       sim\n",
       "173  é muito engraçado o áudio do matheus descreven...       sim\n",
       "174  rt @brenolemes2016: que mina que não gosta de ...       sim\n",
       "175  rt @renatotaioba: assistindo vídeos de receita...       sim\n",
       "176  rt @saarahsaad: odiar ex é mt geração nutella ...       sim\n",
       "177  arthur fez um bolo e levou hoje pro trabalho, ...       sim\n",
       "178  rt @isasantana2403: só queria um morango com n...       sim\n",
       "179  https://t.co/56amzinurr\\nquero muito um pote d...       sim\n",
       "180  rt @alexmapelli0: já fui 10/10 com quem não me...       sim\n",
       "181      eu sou a ex mais nutella que existe kkkkkkkkk       sim\n",
       "182  eu sou uma pessoa tão indecisa que tô comendo ...       sim\n",
       "183  rt @umvicente: iago sousa não está mais entre ...       sim\n",
       "184  gostei de um vídeo @youtube https://t.co/mzaqi...       sim\n",
       "185  eu ganhei 2 bombons de leite ninho com nutella...       sim\n",
       "186  rt @tmtisaac: adolescência nutella// adolescên...       sim\n",
       "187  sdds do meu namorado n deu sinal de vida desde...       sim\n",
       "188  rt @iioveviajar: japão 🇯🇵 https://t.co/ywg73hpglr       sim\n",
       "189  vou fazer pro gabriel de brigadeiro de leite n...       sim\n",
       "190  comi o resto da nutella que tinha aqui, amanhã...       sim\n",
       "191  @kirchprincess vou hitar mais que o irmão do f...       sim\n",
       "192  rt @09h08_: entao vamos viver, e um dia a gent...       sim\n",
       "193  rt @viniceow: mais um dia sem achar o rumo da ...       sim\n",
       "194  rt @ac_bulhoes: essa sua pele preta, nutella, ...       sim\n",
       "195  @aguinaldinho pensamento retrógrado é você cri...       sim\n",
       "196  rt @portalechociay: um pouco de pote de nutell...       sim\n",
       "197  imagina se eu fizesse textão no facebook toda ...       sim\n",
       "198  rt @califaphotos: beijar só fortalece a amizad...       sim\n",
       "199  achei dez conto na rua hoje, amanhã dá pra eu ...       sim\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listatestesimenao = ['sim']*200\n",
    "\n",
    "dfteste2 = {\"Teste\": addmsgs, 'relevante':listatestesimenao }\n",
    "dfteste = pd.DataFrame(dfteste2)\n",
    "dfteste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu Classificador com a base de Testes.<br /><br /> \n",
    "\n",
    "Você deve extrair as seguintes medidas:\n",
    "* Porcentagem de positivos falsos (marcados como relevante mas não são relevantes)\n",
    "* Porcentagem de positivos verdadeiros (marcado como relevante e são relevantes)\n",
    "* Porcentagem de negativos verdadeiros (marcado como não relevante e não são relevantes)\n",
    "* Porcentagem de negativos falsos (marcado como não relevante e são relevantes)\n",
    "\n",
    "Opcionalmente:\n",
    "* Criar categorias intermediárias de relevância baseado na diferença de probabilidades. Exemplo: muito relevante, relevante, neutro, irrelevante e muito irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___\n",
    "## Concluindo\n",
    "\n",
    "Escreva aqui a sua conclusão.<br /> \n",
    "Faça um comparativo qualitativo sobre as medidas obtidas.<br />\n",
    "Explique como são tratadas as mensagens com dupla negação e sarcasmo.<br />\n",
    "Proponha um plano de expansão. Por que eles devem continuar financiando o seu projeto?<br />\n",
    "\n",
    "Opcionalmente: \n",
    "* Discorrer por que não posso alimentar minha base de Treinamento automaticamente usando o próprio classificador, aplicado a novos tweets.\n",
    "* Propor diferentes cenários de uso para o classificador Naive-Bayes. Cenários sem intersecção com este projeto.\n",
    "* Sugerir e explicar melhorias reais no classificador com indicações concretas de como implementar (não é preciso codificar, mas indicar como fazer e material de pesquisa sobre o assunto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
